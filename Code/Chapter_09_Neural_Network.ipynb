{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9. Neural Network\n",
    "##  CPE 490 590\n",
    "### Author: Rahul Bhadani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Two-layer Neural Network on MNIST Dataset\n",
    "\n",
    "### MNIST dataset is a dataset for handwritten digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lrt ../Data/MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set the font parameters\n",
    "plt.rcParams['font.family'] = 'Serif'\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"https://github.com/rahulbhadani/CPE490_590_Sp2025/raw/refs/heads/master/Data/MNIST/mnist_train.csv.gz\", header=None, compression='gzip')\n",
    "\n",
    "# Visualize a few samples\n",
    "def visualize_sample(index):\n",
    "    first_row = df.iloc[index, 1:]\n",
    "    image = np.reshape(first_row.values, (28, 28))\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "visualize_sample(0)\n",
    "visualize_sample(1)\n",
    "visualize_sample(2)\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.iloc[:, 1:].values\n",
    "y = df.iloc[:, 0].values\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y = np.eye(10)[y] \n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors and move to the GPU\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Define the model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 3)\n",
    "        self.fc2 = nn.Linear(3, 4)\n",
    "        self.fc3 = nn.Linear(4, 5)\n",
    "        self.fc4 = nn.Linear(5, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.softmax(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "model = SimpleNN().to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Train the model\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the first column with index 0 is label (digit 5 for first row, digit 0 for the second row, and so on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_test = pd.read_csv('https://raw.githubusercontent.com/rahulbhadani/CPE490_590_Sp2025/refs/heads/master/Data/MNIST/mnist_test.csv', header=None)\n",
    "X_unknown = df_test.iloc[:, 1:].values\n",
    "y_known = df_test.iloc[:, 0].values\n",
    "\n",
    "# Convert to PyTorch tensors and move to the GPU\n",
    "X_unknown = torch.tensor(X_unknown, dtype=torch.float32).to(device)\n",
    "\n",
    "# Predict the labels of the unknown data\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_unknown)\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1).cpu().numpy()\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_known, y_pred_labels)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Find the indices of the accurately predicted samples\n",
    "accurate_indices = np.where(y_pred_labels == y_known)[0]\n",
    "print(\"Accurate indices: \", accurate_indices)\n",
    "\n",
    "# Verify a sample\n",
    "sample_index = 5\n",
    "print(f\"Actual label: {y_known[sample_index]}\")\n",
    "print(f\"Predicted label: {y_pred_labels[sample_index]}\")\n",
    "first_row = df_test.iloc[sample_index, 1:]\n",
    "image = np.reshape(first_row.values, (28, 28))\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Neural Network on Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLength</th>\n",
       "      <th>SepalWidth</th>\n",
       "      <th>PetalLength</th>\n",
       "      <th>PetalWidth</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLength  SepalWidth  PetalLength  PetalWidth        Class\n",
       "0          5.1         3.5          1.4         0.2  Iris-setosa\n",
       "1          4.9         3.0          1.4         0.2  Iris-setosa\n",
       "2          4.7         3.2          1.3         0.2  Iris-setosa\n",
       "3          4.6         3.1          1.5         0.2  Iris-setosa\n",
       "4          5.0         3.6          1.4         0.2  Iris-setosa"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# load dataset\n",
    "Iris = pd.read_csv(\"https://raw.githubusercontent.com/rahulbhadani/CPE490_590_Sp2025/refs/heads/master/Data/Iris/iris.data\")\n",
    "Iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features and Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Iris[[\"SepalLength\", \"SepalWidth\", \"PetalLength\", \"PetalWidth\"]]\n",
    "Labels = Iris[[\"Class\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Training and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train_label, y_test_label = train_test_split(X, Labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features to have mean=0 and variance=1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the scaler \n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "# Scale the train set\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "# Scale the test set\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert class vectors to binary class matrices (for use in categorical_crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape y_train and y_test to be 1D arrays\n",
    "y_train_label = np.ravel(y_train_label)\n",
    "y_test_label = np.ravel(y_test_label)\n",
    "\n",
    "# Now you can fit and transform without warnings\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train_label)\n",
    "y_train = encoder.transform(y_train_label)\n",
    "y_test = encoder.transform(y_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Convert to PyTorch tensors and move to the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Convert to tensors\n",
    "X_train = torch.FloatTensor(X_train).to(device)\n",
    "y_train = torch.LongTensor(y_train).to(device)\n",
    "X_test = torch.FloatTensor(X_test).to(device)\n",
    "y_test = torch.LongTensor(y_test).to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, kaiming_normal_ applies a normal initializer, which initializes the weights of the layer with values drawn from a normal distribution centered on 0, with stddev = sqrt(2 / fan_in), where fan_in is the number of input units in the weight tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (input_layer): Linear(in_features=4, out_features=5, bias=True)\n",
      "  (act1): ReLU()\n",
      "  (hidden_layer1): Linear(in_features=5, out_features=7, bias=True)\n",
      "  (act2): ReLU()\n",
      "  (output_layer): Linear(in_features=7, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty sequential model\n",
    "model = nn.Sequential()\n",
    "\n",
    "# Add layers sequentially\n",
    "model.add_module('input_layer', nn.Linear(4, 5))\n",
    "model.add_module('act1', nn.ReLU())\n",
    "model.add_module('hidden_layer1', nn.Linear(5, 7))\n",
    "model.add_module('act2', nn.ReLU())\n",
    "model.add_module('output_layer', nn.Linear(7, 3))\n",
    "#model.add_module('softmax_layer', nn.Softmax(dim=1))\n",
    "\n",
    "nn.init.kaiming_normal_(model.input_layer.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see the initial Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1897,  0.9292,  0.6941, -1.5096],\n",
       "        [ 0.7600, -0.0213, -0.4207,  0.3085],\n",
       "        [-0.0790,  0.2188,  0.4712, -0.0314],\n",
       "        [-1.0687,  0.5121,  0.2934,  0.3207],\n",
       "        [-0.3466,  0.8713, -0.7504, -0.2401]], requires_grad=True)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.3766,  0.1814,  0.2915,  0.0960,  0.1248,  0.2910, -0.1432],\n",
       "        [ 0.1891, -0.2346,  0.2728, -0.1462, -0.1251, -0.1999, -0.3763],\n",
       "        [-0.2629, -0.0726,  0.2788,  0.3227,  0.2885,  0.1425,  0.1945]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.933333\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    _, predicted = torch.max(test_outputs, 1)\n",
    "    accuracy = (predicted == y_test).sum().item() / y_test.size(0)\n",
    "    print(f'Test accuracy: {accuracy:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Neural Network from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a sample dataset with two features, three training samples $\\mathbf{x} = [ x_1^{(1)}, x_2^{(1)} ], [ x_1^{(2)}, x_2^{(2)} ], [ x_1^{(3)}, x_2^{(3)} ] = [1, 4], [5, 6], [9, 12] $ and response variable $\\mathbf{y} = y^{(1)}, y^{(2)}, y^{(3)} = [-1, 0, 1]$.\n",
    "\n",
    "We want to build a neural networl containing two hidden layer: the first hidden layer will contain 4 hidden units or neurons, the second hidden layer will contain two hidden units or neurons. The prediction will be about predicting class value out of -1, 0, and 1. We will use cross entropy loss and optimization algorithm is stochastic gradient descent with learning rate of 0.05. The activation function to be used is sigmoid function.\n",
    "\n",
    "The following code runs trains the neural network for two epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "\n",
      " Iteration: 0\n",
      "\n",
      "\n",
      "____________________________________________________________\n",
      "\n",
      "\n",
      " Forward Propagation\n",
      "\n",
      "____________________________________________________________\n",
      "\n",
      "Output from first hidden layer:\n",
      " [[0.86989153 0.96442881 0.76852478 0.98901306]\n",
      " [0.97068777 0.99726804 0.96770454 0.99993872]\n",
      " [0.9987706  0.99998763 0.99752738 0.99999999]]\n",
      "Output from second hidden layer:\n",
      " [[0.94406221 0.87537515]\n",
      " [0.95510906 0.88793552]\n",
      " [0.9567904  0.89077129]]\n",
      "Output from output layer:\n",
      " [[0.81883046]\n",
      " [0.8204345 ]\n",
      " [0.82075924]]\n",
      "\n",
      "____________________________________________________________\n",
      "\n",
      "\n",
      " Backward Propagation\n",
      "\n",
      "____________________________________________________________\n",
      "\n",
      "Error:\n",
      " 5.131506429624785\n",
      "Derivative of the error with respect to Wo:\n",
      " [[ 1.81883046]\n",
      " [ 0.8204345 ]\n",
      " [-0.17924076]]\n",
      "Derivative of the error with respect to Wh2:\n",
      " [[ 0.02881505  0.11905354]\n",
      " [ 0.01055302  0.04898291]\n",
      " [-0.00222308 -0.01046384]]\n",
      "Derivative of the error with respect to Wh1:\n",
      " [[ 1.40838368e-02  1.60766787e-03  5.70612263e-03  5.44654432e-04]\n",
      " [ 1.43449994e-03  4.96921004e-05  3.83949244e-04  1.22376619e-06]\n",
      " [-1.32013869e-05 -4.78972716e-08 -6.41918876e-06 -5.85773142e-11]]\n",
      "Updated weights and biases after backpropagation:\n",
      "Wo:\n",
      " [[0.18354012]\n",
      " [0.49195053]]\n",
      "Bo:\n",
      " [0.57699879]\n",
      "Wh2:\n",
      " [[0.59834553 0.89296701]\n",
      " [0.79819544 0.1923398 ]\n",
      " [0.69849301 0.09357707]\n",
      " [0.49815861 0.29218692]]\n",
      "Bh2:\n",
      " [0.49026412 0.59026412]\n",
      "Wh1:\n",
      " [[0.19894312 0.29990722 0.4996216  0.89997246]\n",
      " [0.3967608  0.69966359 0.09874744 0.7998907 ]]\n",
      "Bh1:\n",
      " [0.0988104 0.1988104 0.2988104 0.3988104]\n",
      "\n",
      "============================================================\n",
      "\n",
      " Iteration: 1\n",
      "\n",
      "\n",
      "____________________________________________________________\n",
      "\n",
      "\n",
      " Forward Propagation\n",
      "\n",
      "____________________________________________________________\n",
      "\n",
      "Output from first hidden layer:\n",
      " [[0.86816112 0.96433854 0.76735249 0.98899507]\n",
      " [0.96994152 0.99725802 0.96737171 0.9999386 ]\n",
      " [0.9987082  0.99998755 0.99747849 0.99999999]]\n",
      "Output from second hidden layer:\n",
      " [[0.94311314 0.87121698]\n",
      " [0.95436893 0.88400289]\n",
      " [0.95609851 0.88694846]]\n",
      "Output from output layer:\n",
      " [[0.76471282]\n",
      " [0.76621295]\n",
      " [0.76652924]]\n",
      "\n",
      "____________________________________________________________\n",
      "\n",
      "\n",
      " Backward Propagation\n",
      "\n",
      "____________________________________________________________\n",
      "\n",
      "Error:\n",
      " 4.344869045818073\n",
      "Derivative of the error with respect to Wo:\n",
      " [[ 1.76471282]\n",
      " [ 0.76621295]\n",
      " [-0.23347076]]\n",
      "Derivative of the error with respect to Wh2:\n",
      " [[ 0.01737724  0.09740481]\n",
      " [ 0.00612431  0.03865198]\n",
      " [-0.00179864 -0.01151672]]\n",
      "Derivative of the error with respect to Wh1:\n",
      " [[ 1.11455035e-02  1.12128553e-03  3.79409509e-03  4.03975307e-04]\n",
      " [ 1.11311930e-03  3.36960278e-05  2.49186769e-04  8.80716905e-07]\n",
      " [-1.46561985e-05 -4.54445046e-08 -5.87046179e-06 -5.88816423e-11]]\n",
      "Updated weights and biases after backpropagation:\n",
      "Wo:\n",
      " [[0.07492248]\n",
      " [0.39156525]]\n",
      "Bo:\n",
      " [0.46212604]\n",
      "Wh2:\n",
      " [[0.59738403 0.88743944]\n",
      " [0.79714212 0.18629177]\n",
      " [0.69761977 0.08854472]\n",
      " [0.49708305 0.28601363]]\n",
      "Bh2:\n",
      " [0.48295197 0.58295197]\n",
      "Wh1:\n",
      " [[0.19811416 0.29984275 0.49937224 0.89995204]\n",
      " [0.39420656 0.69942925 0.09791739 0.79980964]]\n",
      "Bh1:\n",
      " [0.09791834 0.19791834 0.29791834 0.39791834]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Cross-Entropy loss and its derivative\n",
    "def cross_entropy_loss(y_actual, y_pred):\n",
    "    return -np.sum(np.multiply(y_actual, np.log(y_pred)) + np.multiply((1 - y_actual), np.log(1 - y_pred)))\n",
    "\n",
    "def cross_entropy_loss_derivative(y_actual, y_pred):\n",
    "    return -(np.divide(y_actual, y_pred) - np.divide(1 - y_actual, 1 - y_pred))\n",
    "\n",
    "# Input samples\n",
    "X = np.array([[1, 4], [5, 6], [9, 12]])\n",
    "\n",
    "# Actual output\n",
    "Y = np.array([[-1], [0], [1]])\n",
    "\n",
    "# Weights and biases initialization from previous example\n",
    "weights_hidden1 = np.array([[0.2, 0.3, 0.5, 0.9], [0.4, 0.7, 0.1, 0.8]])\n",
    "biases_hidden1 = np.array([0.1, 0.2, 0.3, 0.4])\n",
    "weights_hidden2 = np.array([[0.6, 0.9], [0.8, 0.2], [0.7, 0.1], [0.5, 0.3]])\n",
    "biases_hidden2 = np.array([0.5, 0.6])\n",
    "weights_output = np.array([[0.3], [0.6]])\n",
    "biases_output = np.array([0.7])\n",
    "\n",
    "# Learning rate\n",
    "eta = 0.05\n",
    "\n",
    "\n",
    "# Training for 2 iterations\n",
    "for iii in range(2):\n",
    "    # Forward Propagation\n",
    "    print(\"\\n============================================================\")\n",
    "    print(\"\\n Iteration: {}\\n\".format(iii))\n",
    "    print(\"\\n____________________________________________________________\\n\")\n",
    "    print(\"\\n Forward Propagation\")\n",
    "    print(\"\\n____________________________________________________________\\n\")\n",
    "\n",
    "    hidden_layer1_output = sigmoid(np.dot(X, weights_hidden1) + biases_hidden1)\n",
    "    print(\"Output from first hidden layer:\\n\", hidden_layer1_output)\n",
    "\n",
    "    hidden_layer2_output = sigmoid(np.dot(hidden_layer1_output, weights_hidden2) + biases_hidden2)\n",
    "    print(\"Output from second hidden layer:\\n\", hidden_layer2_output)\n",
    "\n",
    "    predicted_output = sigmoid(np.dot(hidden_layer2_output, weights_output) + biases_output)\n",
    "    print(\"Output from output layer:\\n\", predicted_output)\n",
    "\n",
    "    print(\"\\n____________________________________________________________\\n\")\n",
    "\n",
    "    print(\"\\n Backward Propagation\")\n",
    "    print(\"\\n____________________________________________________________\\n\")\n",
    "\n",
    "    # Backward Propagation\n",
    "    error = cross_entropy_loss(Y, predicted_output)\n",
    "    print(\"Error:\\n\", error)\n",
    "\n",
    "    d_predicted_output = cross_entropy_loss_derivative(Y, predicted_output) * sigmoid_derivative(predicted_output)\n",
    "    print(\"Derivative of the error with respect to Wo:\\n\", d_predicted_output)\n",
    "\n",
    "    error_hidden_layer2 = d_predicted_output.dot(weights_output.T)\n",
    "    d_hidden_layer2 = error_hidden_layer2 * sigmoid_derivative(hidden_layer2_output)\n",
    "    print(\"Derivative of the error with respect to Wh2:\\n\", d_hidden_layer2)\n",
    "\n",
    "    error_hidden_layer1 = d_hidden_layer2.dot(weights_hidden2.T)\n",
    "    d_hidden_layer1 = error_hidden_layer1 * sigmoid_derivative(hidden_layer1_output)\n",
    "    print(\"Derivative of the error with respect to Wh1:\\n\", d_hidden_layer1)\n",
    "\n",
    "    # Updating Weights and Biases\n",
    "    weights_output -= eta * hidden_layer2_output.T.dot(d_predicted_output)\n",
    "    biases_output -= eta * np.sum(d_predicted_output)\n",
    "\n",
    "    weights_hidden2 -= eta * hidden_layer1_output.T.dot(d_hidden_layer2)\n",
    "    biases_hidden2 -= eta * np.sum(d_hidden_layer2)\n",
    "\n",
    "    weights_hidden1 -= eta * X.T.dot(d_hidden_layer1)\n",
    "    biases_hidden1 -= eta * np.sum(d_hidden_layer1)\n",
    "\n",
    "    print(\"Updated weights and biases after backpropagation:\")\n",
    "    print(\"Wo:\\n\", weights_output)\n",
    "    print(\"Bo:\\n\", biases_output)\n",
    "    print(\"Wh2:\\n\", weights_hidden2)\n",
    "    print(\"Bh2:\\n\", biases_hidden2)\n",
    "    print(\"Wh1:\\n\", weights_hidden1)\n",
    "    print(\"Bh1:\\n\", biases_hidden1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CPE490590",
   "language": "python",
   "name": "cpe490590"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
